{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo for the Calculation of the Semantic Brand Score - Basic Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Down the Rabbit-Hole. Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it\n"
     ]
    }
   ],
   "source": [
    "# Read text documents from an example CSV file\n",
    "import csv\n",
    "readfile = csv.reader(open(\"AliceWonderland.csv\", 'rt',  encoding=\"utf8\"), delimiter = \"|\", quoting=csv.QUOTE_NONE)\n",
    "texts = [line[0] for line in readfile]\n",
    "#4 Chapters of Alice in Wonderland\n",
    "print(len(texts))\n",
    "print(texts[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I <font color=\"#d81e5b\">**imported the text file**</font> in Python as a list of text documents (texts), which are processed to remove punctuation, stop-words and special characters. Words are lowercased and split into tokens, thus obtaining a new texts variable, which is a list of lists. More complex operations of <font color=\"#d81e5b\">**text preprocessing**</font> are always possible (such as the removal of html tags or ‘#’), for which I recommend reading one of many tutorials on Natural Language Processing in Python. The stopwords list is taken from the NLTK package. Lastly, word affixes are remove through Snowball Stemming (lemming or other approaches would be possible, also depending on the language of texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rabbit', 'hole', 'alice', 'begin', 'get', 'tire']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Import re, string and nltk, and download stop-words\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Define stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "stopw = nltk.corpus.stopwords.words('english') + ['‘']\n",
    "\n",
    "#Define brands (lowercase)\n",
    "brands = ['alice', 'rabbit']\n",
    "\n",
    "# texts is a list of strings, one for each document analyzed.\n",
    "\n",
    "#Convert to lowercase\n",
    "texts = [t.lower() for t in texts]\n",
    "#Remove words that start with HTTP\n",
    "texts = [re.sub(r\"http\\S+\", \" \", t) for t in texts]\n",
    "#Remove words that start with WWW\n",
    "texts = [re.sub(r\"www\\S+\", \" \", t) for t in texts]\n",
    "#Remove punctuation\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "texts = [regex.sub(' ', t) for t in texts]\n",
    "#Remove words made of single letters\n",
    "texts = [re.sub(r'\\b\\w{1}\\b', ' ', t) for t in texts]\n",
    "#Remove stopwords\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopw) + r')\\b\\s*')\n",
    "texts = [pattern.sub(' ', t) for t in texts]\n",
    "#Remove additional whitespaces\n",
    "texts = [re.sub(' +',' ',t) for t in texts]\n",
    "\n",
    "#Tokenize text documents (becomes a list of lists)\n",
    "texts = [t.split() for t in texts]\n",
    "\n",
    "# Snowball Stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "texts = [[stemmer.stem(w) if w not in brands else w for w in t] for t in texts]\n",
    "texts[0][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During text preprocessing we should pay attention not to lose useful information. Smileys :-), made of punctuation, can be very important if we calculate sentiment.\n",
    "We can now proceed with the <font color=\"#d81e5b\">**calculation of prevalence**</font>, which counts the frequency of occurrence of each brand name —  <font color=\"#d81e5b\">**subsequently standardized**</font> considering the scores of all the words in the texts. My choice of standardization here is to subtract the mean and divide by the standard deviation. Other approaches are also possible. This step is important to compare measures carried out considering different time frames or sets of documents (e.g. brand importance on Twitter in April and May). Normalization of absolute scores is necessary before summing prevalence, diversity and connectivity to obtain the Semantic Brand Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence alice 6.716253083416179\n",
      "Prevalence rabbit 0.23502089447171487\n"
     ]
    }
   ],
   "source": [
    "#PREVALENCE\n",
    "#Import Counter and Numpy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "#Create a dictionary with frequency counts for each word\n",
    "countPR = Counter()\n",
    "for t in texts:\n",
    "    countPR.update(Counter(t))\n",
    "\n",
    "#Calculate average score and standard deviation\n",
    "avgPR = np.mean(list(countPR.values()))\n",
    "stdPR = np.std(list(countPR.values()))\n",
    "\n",
    "#Calculate standardized Prevalence for each brand\n",
    "PREVALENCE = {}\n",
    "for brand in brands:\n",
    "    PREVALENCE[brand] = (countPR[brand] - avgPR) / stdPR\n",
    "    print(\"Prevalence\", brand, PREVALENCE[brand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next and most important step is to <font color=\"#d81e5b\">**transform texts (list of lists of tokens) into a social network where nodes are words and links are weighted according to the number of co-occurrences between each pair of words**</font>. In this step we have to define a co-occurrence range, i.e. a maximum distance between co-occurring words (here it is set to 3). In addition, we might want to <font color=\"#d81e5b\">**remove links which represent negligible co-occurrences**</font>, for example those of weight = 1. It can also be useful to remove isolates, if these are not brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Network\n",
      "No. of Nodes: 1051 No. of Edges: 14455\n",
      "Filtered Network\n",
      "No. of Nodes: 691 No. of Edges: 2710\n"
     ]
    }
   ],
   "source": [
    "#Import Networkx\n",
    "import networkx as nx\n",
    "\n",
    "#Choose a co-occurrence range\n",
    "co_range = 5\n",
    "\n",
    "#Create an undirected Network Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "#Each word is a network node\n",
    "nodes = set([item for sublist in texts for item in sublist])\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "#Add links based on co-occurrences\n",
    "for doc in texts:\n",
    "    w_list = []\n",
    "    length= len(doc)\n",
    "    for k, w in enumerate(doc):\n",
    "        #Define range, based on document length\n",
    "        if (k+co_range) >= length:\n",
    "            superior = length\n",
    "        else:\n",
    "            superior = k+co_range+1\n",
    "        #Create the list of co-occurring words\n",
    "        if k < length-1:\n",
    "            for i in range(k+1,superior):\n",
    "                linked_word = doc[i].split()\n",
    "                w_list = w_list + linked_word\n",
    "        #If the list is not empty, create the network links\n",
    "        if w_list:    \n",
    "            for p in w_list:\n",
    "                if G.has_edge(w,p):\n",
    "                    G[w][p]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(w, p, weight=1)\n",
    "        w_list = []\n",
    "\n",
    "#Remove negligible co-occurrences based on a filter\n",
    "link_filter = 2\n",
    "#Create a new Graph which has only links above\n",
    "#the minimum co-occurrence threshold\n",
    "G_filtered = nx.Graph() \n",
    "G_filtered.add_nodes_from(G)\n",
    "for u,v,data in G.edges(data=True):\n",
    "    if data['weight'] >= link_filter:\n",
    "        G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "\n",
    "#Optional removal of isolates\n",
    "isolates = set(nx.isolates(G_filtered))\n",
    "isolates -= set(brands)\n",
    "G_filtered.remove_nodes_from(isolates)\n",
    "\n",
    "#Check the resulting graph (for small test graphs)\n",
    "#G_filtered.nodes()\n",
    "#G_filtered.edges(data = True)\n",
    "print(\"Original Network\\nNo. of Nodes:\", G.number_of_nodes(), \"No. of Edges:\", G.number_of_edges())\n",
    "print(\"Filtered Network\\nNo. of Nodes:\", G_filtered.number_of_nodes(), \"No. of Edges:\", G_filtered.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having determined the co-occurrence network, we can now <font color=\"#d81e5b\">**calculate diversity and connectivity**</font>, which are <font color=\"#d81e5b\">**distinctiveness centrality**</font> (previously we used degree) and weighted betweenness centrality of a brand node. We standardize these values as we did with prevalence. More information about distinctiveness centrality is given <font color=\"#d81e5b\">[**in this paper**](https://arxiv.org/abs/1912.03391). **You will also need to install the Python *distinctivenss* package.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Loops will be ignored.\n",
      "Diversity alice 6.016090810210237\n",
      "Diversity rabbit 0.3527287505665111\n"
     ]
    }
   ],
   "source": [
    "#INSTALL AND IMPORT THE DISTINCTIVENESS PACKAGE\n",
    "#pip install -U distinctiveness\n",
    "from distinctiveness.dc import dc_all\n",
    "\n",
    "#DIVERSITY\n",
    "#Calculate Distinctiveness Centrality\n",
    "DC = dc_all(G_filtered, normalize = False, alpha = 1)\n",
    "DIVERSITY_sequence=DC[\"D2\"]\n",
    "#Calculate average score and standard deviation\n",
    "avgDI = np.mean(list(DIVERSITY_sequence.values()))\n",
    "stdDI = np.std(list(DIVERSITY_sequence.values()))\n",
    "#Calculate standardized Diversity for each brand\n",
    "DIVERSITY = {}\n",
    "for brand in brands:\n",
    "    DIVERSITY[brand] = (DIVERSITY_sequence[brand] - avgDI) / stdDI\n",
    "    print(\"Diversity\", brand, DIVERSITY[brand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate connectivity as weighted betweenness centraliy, we first have to define <font color=\"#d81e5b\">**inverse weights**</font>, as weights are treated by Networkx as distances (which is the opposite of what we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connectivity alice 1.1685215814353405\n",
      "Connectivity rabbit 0.09234660192732538\n"
     ]
    }
   ],
   "source": [
    "#Define inverse weights \n",
    "for u,v,data in G_filtered.edges(data=True):\n",
    "    if 'weight' in data and data['weight'] != 0:\n",
    "        data['inverse'] = 1/data['weight']\n",
    "    else:\n",
    "        data['inverse'] = 1   \n",
    "\n",
    "#CONNECTIVITY\n",
    "CONNECTIVITY_sequence=nx.betweenness_centrality(G_filtered, normalized=False, weight ='inverse')\n",
    "#Calculate average score and standard deviation\n",
    "avgCO = np.mean(list(CONNECTIVITY_sequence.values()))\n",
    "stdCO = np.std(list(CONNECTIVITY_sequence.values()))\n",
    "#Calculate standardized Prevalence for each brand\n",
    "CONNECTIVITY = {}\n",
    "for brand in brands:\n",
    "    CONNECTIVITY[brand] = (CONNECTIVITY_sequence[brand] - avgCO) / stdCO\n",
    "    print(\"Connectivity\", brand, CONNECTIVITY[brand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font color=\"#d81e5b\">**Semantic Brand Score**</font> of each brand is finally obtained by summing the standardized values of prevalence, diversity and connectivity. Different approaches are also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBS alice 13.900865475061757\n",
      "SBS rabbit 0.6800962469655514\n"
     ]
    }
   ],
   "source": [
    "#Obtain the Semantic Brand Score of each brand\n",
    "SBS = {}\n",
    "for brand in brands:\n",
    "    SBS[brand] = PREVALENCE[brand] + DIVERSITY[brand] + CONNECTIVITY[brand]\n",
    "    print(\"SBS\", brand, SBS[brand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREVALENCE</th>\n",
       "      <th>DIVERSITY</th>\n",
       "      <th>CONNECTIVITY</th>\n",
       "      <th>SBS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>6.716253</td>\n",
       "      <td>6.016091</td>\n",
       "      <td>1.168522</td>\n",
       "      <td>13.900865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rabbit</th>\n",
       "      <td>0.235021</td>\n",
       "      <td>0.352729</td>\n",
       "      <td>0.092347</td>\n",
       "      <td>0.680096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PREVALENCE  DIVERSITY  CONNECTIVITY        SBS\n",
       "alice     6.716253   6.016091      1.168522  13.900865\n",
       "rabbit    0.235021   0.352729      0.092347   0.680096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a final pandas data frame with all results\n",
    "import pandas as pd\n",
    "\n",
    "PREVALENCE = pd.DataFrame.from_dict(PREVALENCE, orient=\"index\", columns = [\"PREVALENCE\"])\n",
    "DIVERSITY = pd.DataFrame.from_dict(DIVERSITY, orient=\"index\", columns = [\"DIVERSITY\"])\n",
    "CONNECTIVITY = pd.DataFrame.from_dict(CONNECTIVITY, orient=\"index\", columns = [\"CONNECTIVITY\"])\n",
    "SBS = pd.DataFrame.from_dict(SBS, orient=\"index\", columns = [\"SBS\"])\n",
    "\n",
    "SBS = pd.concat([PREVALENCE, DIVERSITY, CONNECTIVITY, SBS], axis=1, sort=False)\n",
    "SBS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
